{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_MarinaSmirnova.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6St2gKeKgy3"
      },
      "source": [
        "#  English-to-Clips programming language translator. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDVgGi9fRf19"
      },
      "source": [
        "English variants for *deftemplate*:\n",
        "- __ template has properties..\n",
        "- Template \"__\" has properties..\n",
        "- Create a __ template.. \n",
        "- __ has properties\n",
        "\n",
        "\n",
        "English variants for *assert*:\n",
        "- There exists..\n",
        "- Assert..\n",
        "- Add a fact about../Add to the base..\n",
        "\n",
        "\n",
        "English variants for *defrule*:\n",
        "- If..\n",
        "- When.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDSBqVTCKuWL"
      },
      "source": [
        "Examples:\n",
        "- Cat template has properties of color, age, and name.\n",
        "\n",
        "*(deftemplate cat (slot color) (slot age) (slot name))*\n",
        "\n",
        "- There exists a cat with the name Bob.\n",
        "\n",
        "*(assert (cat (name “Bob”)))*\n",
        "\n",
        "- If there exists cat named Bob then there exists a cat named Tom.\n",
        "\n",
        "*(defrule rule1 (cat (name “Bob”)) => (assert (cat (name “Tom”))))*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nGzd_zFBfhs"
      },
      "source": [
        "### Algorithm:\n",
        "1. ask for a sentence \n",
        "2. check grammar and syntax of the sentence\n",
        "3. imply semantics from a given sentence\n",
        "  - tokenize the sentence\n",
        "  - determine the type of the sentence (template/assertion/rule)\n",
        "4. produce a translated expression "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXQLnQbFZgb8"
      },
      "source": [
        "### MVP specifications\n",
        "\n",
        "1. implement translation into 3 basic CLIPS statements: defrule, deftemplate, assert\n",
        "\n",
        "2. realize the idea (not a wholly hardcoded mapping of words): use tokens, compose a grammar, apply parsing\n",
        "\n",
        "3. write clean code and necessary documentation\n",
        "\n",
        "4. include grammar/syntax check of the given sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kRtbs-DLkBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16038047-f475-4c4a-ee56-6faef4a6f4fc"
      },
      "source": [
        "!pip install spacy beautifulsoup4\n",
        "!pip install language-tool-python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.6.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.5.5-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from language-tool-python) (4.62.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from language-tool-python) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->language-tool-python) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->language-tool-python) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->language-tool-python) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->language-tool-python) (2021.5.30)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWHB8GGhcmQ6"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_web_sm\n",
        "import language_tool_python\n",
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDwkQvY4f26Y"
      },
      "source": [
        "# Functions for analysis of tokens\n",
        "\n",
        "def check_for_defrule(tokens):\n",
        "  # if..\n",
        "  # when..\n",
        "  # NOTE: for now, the word \"then\" is obligatory in the inintial sentence\n",
        "  presence_main_clause = False\n",
        "  presence_cond_clause = False\n",
        "  for token in tokens:\n",
        "    if (token.text.casefold() == 'if') or (token.text.casefold() == 'when'):\n",
        "      presence_cond_clause = True\n",
        "    if (token.text.casefold() == 'then'):\n",
        "      presence_main_clause = True\n",
        "  if (presence_main_clause and presence_cond_clause):\n",
        "    return 'defrule'\n",
        "    \n",
        "  return 'undefined'\n",
        "\n",
        "\n",
        "def check_for_deftemplate(tokens):\n",
        "  #- __ template has properties..\n",
        "  #- Template \"__\" has properties..\n",
        "  #- Create a __ template.. \n",
        "  #- __ has properties  \n",
        "  presence_template_word = False\n",
        "  presence_property_word = False\n",
        "  for token in tokens:\n",
        "    if (token.text.casefold() == 'template'):\n",
        "      presence_template_word = True\n",
        "    if (token.text.casefold() == 'property') or (token.text.casefold() == 'properties'):\n",
        "      presence_property_word = True\n",
        "  if (presence_template_word or presence_property_word):\n",
        "    return 'deftemplate'\n",
        "    \n",
        "  return 'undefined'\n",
        "\n",
        "\n",
        "def check_for_assert(tokens):\n",
        "  #- There exists..\n",
        "  #- Assert..\n",
        "  #- Add a fact about../Add to the base.\n",
        "  # combination of (subject+verb) or just verbs\n",
        "  presence_subject = False\n",
        "  presence_verb = False\n",
        "  for token in tokens:\n",
        "    if (token.dep_ == 'nsubj') and (token.head.text is not None):\n",
        "      presence_subject = True\n",
        "      presence_verb = True\n",
        "    if ('VB' in token.tag_ ):\n",
        "      presence_verb = True\n",
        "  if (presence_subject or presence_verb):\n",
        "    return 'assert'\n",
        "    \n",
        "  return 'undefined'  \n",
        "\n",
        "\n",
        "def determine_sentence_type(tokens):\n",
        "  #1. check for patterns of DEFRULE\n",
        "  #2. check for patterns of DEFTEMPLATE\n",
        "  #3. check for pattern of ASSERT (presence of subject+verb)\n",
        "  #4. otherwise \"undefined type\"\n",
        "  sent_type = 'undefined'\n",
        "  sent_type = check_for_defrule(tokens)\n",
        "  if sent_type == 'undefined':\n",
        "    sent_type = check_for_deftemplate(tokens)\n",
        "    if sent_type == 'undefined':\n",
        "      sent_type = check_for_assert(tokens)\n",
        "\n",
        "  return sent_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzDl0vlNmMo6"
      },
      "source": [
        "# Functions for parsing tokens\n",
        "\n",
        "\n",
        "def extract_subject_verb_pairs(tokens):\n",
        "  # 1. search through noun chunks\n",
        "  # 2. find subjects - chunks with 'nsubj' as a root; -> assign them to actors in the separate facts\n",
        "  # 3. find corresponding verbs for subjects \n",
        "  # NOTE: Sentences of type There is/are do NOT have subjects, instead they have attributes attached to verbs\n",
        "  # Also, the order of words in facts is: (verb, subject, attribute/object)\n",
        "  subject_verb_pairs = []\n",
        "  enhanced_pairs = []\n",
        "  for chunk in tokens.noun_chunks:\n",
        "      if (chunk.root.dep_ == 'nsubj'):\n",
        "        subject_verb_pairs.append([chunk.root.head.text, chunk.root.text]) #verb, subject\n",
        "        enhanced_pairs.append([chunk.root.head.text, chunk.root.text])\n",
        "      if (chunk.root.dep_ == 'attr'):\n",
        "        subject_verb_pairs.append([chunk.root.head.text, chunk.root.text]) #verb, attribute\n",
        "        enhanced_pairs.append([chunk.root.head.text, chunk.root.text])\n",
        "      if (chunk.root.dep_ == 'dobj'):\n",
        "        i = len(enhanced_pairs)\n",
        "        for pair in reversed(enhanced_pairs): \n",
        "          i -= 1\n",
        "          if (pair[0] == chunk.root.head.text):\n",
        "            enhanced_pairs[i].append(chunk.root.text)\n",
        "            break\n",
        "  return subject_verb_pairs\n",
        "\n",
        "\n",
        "def enhance_pairs_with_objects(tokens, subject_verb_pairs):\n",
        "  # attach objects to corresponsing pairs (verb+subject)\n",
        "  # Example: (have+I) <- apple\n",
        "  enhanced_pairs = copy.deepcopy(subject_verb_pairs)\n",
        "  for chunk in tokens.noun_chunks:\n",
        "    if (chunk.root.dep_ == 'dobj'):\n",
        "        i = len(enhanced_pairs)\n",
        "        for pair in reversed(enhanced_pairs): \n",
        "          i -= 1\n",
        "          if (pair[0] == chunk.root.head.text):\n",
        "            enhanced_pairs[i].append(chunk.root.text)\n",
        "            break\n",
        "  return enhanced_pairs\n",
        "\n",
        "\n",
        "def enhance_pairs_with_adjectives(tokens, enhanced_pairs):\n",
        "  # attach adjecctives to corresponsing pairs (verb+subject)\n",
        "  # Example: (am+I) <- cool\n",
        "  enhanced_pairs_v2 = copy.deepcopy(enhanced_pairs)\n",
        "  start = 0\n",
        "  for i in range(len(list(tokens))):\n",
        "    if (tokens[i].dep_ == 'acomp'):\n",
        "      for j in range(start, len(enhanced_pairs_v2)):\n",
        "        if (enhanced_pairs_v2[j][0] == tokens[i].head.text):\n",
        "          enhanced_pairs_v2[j].append(tokens[i].text)\n",
        "          start += 1\n",
        "          break\n",
        "  return enhanced_pairs_v2\n",
        "\n",
        "\n",
        "def extract_facts(tokens):\n",
        "  # Extraction of facts(parsing) is done with the sequence of steps:\n",
        "  # 1. find pairs (action+actor)\n",
        "  # 2. enrich the pairs with corresponding objects\n",
        "  # 3. enrich the pairs with adjectival complements\n",
        "  subject_verb_pairs = extract_subject_verb_pairs(tokens)\n",
        "  enhanced_pairs_v1 = enhance_pairs_with_objects(tokens, subject_verb_pairs)\n",
        "  enhanced_pairs_v2 = enhance_pairs_with_adjectives(tokens, enhanced_pairs_v1)\n",
        "  return enhanced_pairs_v2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niTav8YQgoyW"
      },
      "source": [
        "def extract_defrule_details(tokens):\n",
        "  # List of the details:\n",
        "  # 1. explicitly given rule's name (not implemented in MVP)\n",
        "  # 2. facts in conditional part \n",
        "  # 3. facts in action part \n",
        "  details = {}\n",
        "  # assigning default 'name-of-the-rule'\n",
        "  details['name'] = 'name-of-the-rule'\n",
        "\n",
        "  # 1. determine which part of the sentence (1st or 2nd) is conditional\n",
        "  # 2. determine which facts from conditional part and which ones from action part (mark verbs from the facts accordingly) \n",
        "  details['conditional_verbs'] = []\n",
        "  details['action_verbs'] = []\n",
        "  if (tokens[0].text == 'If' ) or (tokens[0].text == 'When'):\n",
        "    details['condition'] = 1\n",
        "    for token in tokens:\n",
        "      if (token.text == 'then'):\n",
        "        break\n",
        "      if ('VB' in token.tag_ ):\n",
        "        details['conditional_verbs'].append(token.text)\n",
        "\n",
        "  if (tokens[0].text == 'Then' ):\n",
        "    details['condition'] = 2 \n",
        "    for token in tokens:\n",
        "      if (token.text == 'if') or (token.text == 'when'):\n",
        "        break\n",
        "      if ('VB' in token.tag_ ):\n",
        "        details['action_verbs'].append(token.text)\n",
        "\n",
        "  return details\n",
        "\n",
        "\n",
        "def extract_deftemplate_details(tokens):\n",
        "  # List of the details:\n",
        "  # 1. template's name\n",
        "  # 2. slot names\n",
        "\n",
        "  # Two types of sentences supported for MVP:\n",
        "  #1 - __ template has..\n",
        "  #2 - __ has.. \n",
        "\n",
        "  # Also, only one template per sentence is supported for MVP\n",
        "  details = {}\n",
        "  details['name'] = ''\n",
        "  #look for word expression \"__ template\"\n",
        "  for token in tokens:\n",
        "    if (token.dep_ == 'compound') and (token.head.text == 'template'):\n",
        "      details['name'] = token.text\n",
        "  if (details['name'] == ''):\n",
        "    #or assign subject as a name \n",
        "    for token in tokens:\n",
        "      if (token.dep_ == 'subj'):\n",
        "        details['name'] = token.text\n",
        "        break\n",
        "\n",
        "  #To extract properties:\n",
        "  # 1. Find the first noun (pobj or)\n",
        "  # 2. Iterate over the next tokens while they are delimited with ','/'and' + they are nouns\n",
        "\n",
        "  #NOTE: there are two variants:\n",
        "  # 1 - there is a collocation 'property of'/'properties of'\n",
        "  # 2 - otherwise\n",
        "  details['slots'] = []\n",
        "  for i in range(1, len(list(tokens))):\n",
        "    if ((tokens[i].text == 'of') and (tokens[i-1].text == 'properties' or tokens[i].text == 'property')) \\\n",
        "        or (tokens[i].text == 'have' or tokens[i].text == 'has'):\n",
        "      j = i+1\n",
        "      while (j < len(list(tokens))) and (tokens[j].pos_ == 'NOUN' or tokens[j].pos_ == 'PUNCT' or tokens[j].pos_ == 'CCONJ'):\n",
        "        if (tokens[j].pos_ == 'NOUN'):\n",
        "          details['slots'].append(tokens[j].text)\n",
        "        j += 1\n",
        "      break \n",
        "\n",
        "  return details\n",
        "\n",
        "\n",
        "def extract_assert_details(tokens):\n",
        "  # List of the details:\n",
        "  details = {}\n",
        "  return details\n",
        "\n",
        "\n",
        "def extract_specifications(sentence_type, tokens):\n",
        "  # Extract specific word expressions (keywords, commands) for each type of the sentence\n",
        "  if sentence_type == 'defrule':\n",
        "    return extract_defrule_details(tokens)\n",
        "  if sentence_type == 'deftemplate':\n",
        "    return extract_deftemplate_details(tokens) \n",
        "  if sentence_type == 'assert':\n",
        "    return extract_assert_details(tokens)\n",
        "\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKr6wV6ybifU"
      },
      "source": [
        "def form_defrule(specifications, facts):\n",
        "  # (defrule rule1 (cat (name “Bob”)) => (assert (cat (name “Tom”))))\n",
        "  translated_sentence = '(defrule ' + specifications['name'] + '\\n    '\n",
        "  # will call translate_to_assert(specifications, facts)\n",
        "\n",
        "  num_cond = len(specifications['conditional_verbs'])\n",
        "  num_action = len(specifications['action_verbs'])\n",
        "  start = 0\n",
        "  end = 0\n",
        "  if (specifications['condition'] == 1):\n",
        "    start = 0\n",
        "    end = num_cond\n",
        "  else:\n",
        "    start = num_action\n",
        "    end = len(facts)\n",
        "\n",
        "  for i in range(start, end):\n",
        "      fact = ' '.join(facts[i])\n",
        "      translated_sentence = translated_sentence + '(' + fact + ')\\n    ' \n",
        "\n",
        "  translated_sentence = translated_sentence + '=>\\n    '\n",
        "\n",
        "  if (specifications['condition'] == 2):\n",
        "    start = 0\n",
        "    end = num_action\n",
        "  else:\n",
        "    start = num_cond\n",
        "    end = len(facts)\n",
        "\n",
        "  for i in range(start, end):\n",
        "    fact = ' '.join(facts[i])\n",
        "    translated_sentence = translated_sentence + '(assert (' + fact + '))\\n    ' \n",
        "\n",
        "  return translated_sentence\n",
        "\n",
        "\n",
        "def form_deftemplate(specifications, facts):\n",
        "  # (deftemplate cat (slot color) (slot age) (slot name)) \n",
        "  translated_sentence = '(deftemplate ' + specifications['name'] + '\\n    '\n",
        "  num_slots = len(specifications['slots'])\n",
        "  for i in range(num_slots-1):\n",
        "    translated_sentence = translated_sentence + '(slot ' + specifications['slots'][i] +  ')\\n    '\n",
        "\n",
        "  translated_sentence = translated_sentence + '(slot ' + specifications['slots'][num_slots-1] + '))'\n",
        "  return translated_sentence\n",
        "\n",
        "\n",
        "def form_assert(specifications, facts):\n",
        "  # (assert (cat (name “Bob”)))\n",
        "  translated_sentences = []\n",
        "  for fact in facts:\n",
        "    fact_str = ' '.join(fact)\n",
        "    sentence = '(assert (' + fact_str + '))'\n",
        "    translated_sentences.append(sentence) \n",
        "\n",
        "  return translated_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW2oW6KpKbza"
      },
      "source": [
        "class Translator(object):\n",
        "\n",
        "  def tokenize(self, sentence):\n",
        "    # divide the sentence into predefined tokens\n",
        "    nlp = en_core_web_sm.load()\n",
        "    doc = nlp(sentence)\n",
        "    return doc\n",
        "\n",
        "\n",
        "  def analyse_tokens(self, tokens):\n",
        "    result = []\n",
        "    sentence_type = determine_sentence_type(tokens)\n",
        "    if sentence_type == 'undefined':\n",
        "      return 'Sorry, the translator does not support such kind of the sentence.'\n",
        "\n",
        "    facts = extract_facts(tokens)\n",
        "    specifications = extract_specifications(sentence_type, tokens)\n",
        "\n",
        "    if sentence_type == 'defrule':\n",
        "      result = form_defrule(specifications, facts)\n",
        "    if sentence_type == 'deftemplate':\n",
        "      result = form_deftemplate(specifications, facts)\n",
        "    if sentence_type == 'assert':\n",
        "      result = form_assert(specifications, facts)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "  def translate(self, sentence):\n",
        "    # integrate all translation steps\n",
        "    tokens = self.tokenize(sentence)\n",
        "    translated_sentence = self.analyse_tokens(tokens)\n",
        "    return translated_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNO5JSCKUKzI"
      },
      "source": [
        "def ask_for_sentence(tool):\n",
        "  # ask for a sentence from the user\n",
        "  sentence = \"\"\n",
        "\n",
        "  # check the sentence for mistakes in grammar/syntax\n",
        "  while True:\n",
        "    sentence = input('Enter a sentence in English: ')\n",
        "    matches = tool.check(sentence)    \n",
        "    if (len(matches) == 0):\n",
        "      break\n",
        "    print('The entered sentence has incorrect grammar or syntax. Please try again.')\n",
        "  return sentence\n",
        "\n",
        "\n",
        "def print_translation(translated_sentence):\n",
        "  if isinstance(translated_sentence, list):\n",
        "    for sentences in translated_sentence:\n",
        "      print(sentences)\n",
        "  else:\n",
        "    print(translated_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq4_ejUIFSp-"
      },
      "source": [
        "## A set of English sentences supported for translation by the MVP version\n",
        "**To define a \"defrule\" provide a sentence:**\n",
        "- with conditional part starting with \"if\"/\"when\"\n",
        "- with actional part starting with \"then\"\n",
        "- there can be multiple facts in both parts\n",
        "\n",
        "**To define a \"deftemplate\" provide a sentence:**\n",
        "- with a single template's definition\n",
        "- with template's name as the following examples:\n",
        "  - *template-name* template has..\n",
        "  - *template-name* has.. \n",
        "- with at least one word from the list: \"template\", \"property\", \"properties\" (to correctly determine the type of the sentence). \n",
        "\n",
        "\n",
        "**To define an \"assert\" provide a sentence:**\n",
        "- see Notes below\n",
        "- in case of multiple facts in one sentence, they will be split into multiple \"assert\" statements \n",
        "\n",
        "### Notes:\n",
        "- homogeneous subjects are not supported (e.g. ~Alice and Tom sing.~)\n",
        "- homogeneous actions are not supported (e.g. ~I sing and walk.~)\n",
        "- homogeneous adjectives are not supported (e.g. ~I am happy and calm.~))\n",
        "- the order of words in the produced translation: *(verb subject object/adjective)*. For example, (assert (have I apple)). Taken from some tutorial.\n",
        "- construction of \"assert\" facts with multiple attributes is not supported (e.g. ~(assert (person (name Bob) (age 20)))~) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGORQQB7TXFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb297c2b-da8b-41c2-b7ef-ebf9c47ecccf"
      },
      "source": [
        "translator = Translator()\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "sentence = \"\"\n",
        "\n",
        "while (sentence != 'stop'):\n",
        "  sentence = ask_for_sentence(tool)\n",
        "  if sentence == 'stop': \n",
        "    break\n",
        "  translated_sentence = translator.translate(sentence)\n",
        "  print_translation(translated_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading LanguageTool: 100%|██████████| 203M/203M [00:11<00:00, 17.7MB/s]\n",
            "Unzipping /tmp/tmpwbfihrlj.zip to /root/.cache/language_tool_python.\n",
            "Downloaded https://www.languagetool.org/download/LanguageTool-5.4.zip to /root/.cache/language_tool_python.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(assert (have I apple))\n",
            "The entered sentence has incorrect grammar or syntax. Please try again.\n",
            "(defrule name-of-the-rule\n",
            "    (is it rainy)\n",
            "    =>\n",
            "    (assert (walk I))\n",
            "    \n",
            "(deftemplate Cat\n",
            "    (slot name))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX1Q-mUP5PI1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}